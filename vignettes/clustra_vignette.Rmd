---
title: "clustra: clustering trajectories"
author: "George Ostrouchov, Hanna Gerlovin, and David Gagnon"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{clustra}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```
The **clustra** package was built to cluster medical trajectories related by an intervention. For example, a number of individuals are started on a specific drug regimen and their blood pressure data is collected for a varying amount of time before and after the start of the medication. Time is recorded as negative before the start and positive after the start, meaning that the trajectories are aligned at zero, irrespective of any reference to calendar time. Time units can be anything (days, weeks, hours, minutes, etc.) as long as they are the same across all patients. Observations can be unequally spaced.  

First, we load the package and set up a directory where we want to have any output from this vignette.  
```{r setup}
library(clustra)

## Set working directory
playdir = "~/clustra_play"
if(!dir.exists(playdir)) dir.create(playdir)
```
We begin by generating a data set. The parameters generate *id*s and for each *id*, a random number of observations based on the *Poisson*($\lambda =$ *m_obs*) distribution plus 3. The 3 additional observations are to guarantee one before intervention at time *start*, one at the intervention time 0, and one after the intervention at time *end*. The *start* time is *Uniform(s_range)* and the *end* time is *Uniform(e_range)*. The remaining times are at times Uniform(*start*, *end*). The time units are arbitrary and depend on your application.

We also set a seed for reproducibility, look at a few observations of the generated data, and write the data to a csv file.
```{r}
set.seed(1234567)
data = gen_traj_data(n_id = 2000, m_obs = 25, s_range = c(-50, -10),
                     e_range = c(3*365, 10*365), reference = 100)
head(data)
write.table(data, file = "clustra_gen_data.csv")
```
Select a few random *id*s and print their scatterplots.
```{r fig.width = 7, fig.height = 9}
library(ggplot2)
iplot = sample(unique(data$id), 9)
sampobs = match(data$id, iplot, nomatch = 0) > 0
ggplot(data[sampobs, ], aes(x = time, y = response)) +
  facet_wrap(~ id) + geom_point()
```
Next, cluster the trajectories. Set RNGkind and seed for reproducibility. Set *k=3*, spline max degrees of freedom to 30, maximum iterations to 10, use best of 5 random starts to initialize clusters, use 20 ids to quickly evaluate the random starts, and retry up to 3 times when an initialization fails to have enough data in a cluster to estimate the spline. *cores* parameters set the number of cores to use in various components of the code. Note that this does not work on Windows operating systems, where it should be left at 1. We proceed with verbose output to get starts and change counts from iterations.
```{r}
rng_prev = RNGkind("L'Ecuyer-CMRG")
set.seed(1234737)
cl = clustra(data, k = 3, fp = list(maxdf = 30, iter = 10, starts = 5, idperstart = 20, retry_max = 3), cores = c(e_mc = 1, m_mc = 1, nthreads = 1, blas = 1), verbose = TRUE)
```

Next, plot the resulting fit with a sample of data, colored by the cluster value.
```{r fig.width = 7, fig.height = 7}
sdt = data[, group:=factor(..cl$data_group)][sample(nrow(data), 10000)]
np = 100
k = length(cl$tps)
ntime = seq(data[, min(time)], data[, max(time)], length.out = np)
pdata = expand.grid(time = ntime, group = factor(1:k))
pdata$pred = do.call(c, lapply(cl$tps, predict, newdata = list(time = ntime),
                         type = "response"))
ggplot(pdata, aes(x = time, y = pred, color = group)) + geom_line() +
  geom_point(data = sdt, aes(y = response), pch = ".") 
```
Average silhouette value is a way to select the number of clusters and a silhouette plot provides a way for a deeper evaluation (Rouseeuw 1986). As silhouette requires distances between trajectories, this is not possible due to unequal trajectory sampling without fitting a separate model for each id. As a proxy for distance between points, we use point distances to cluster means in the *clustra_sil()* function. The structure returned from the *clustra()* function contains the matrix *loss*, which has all the information needed to construct these proxy silhouette plots. The function *clustra_sil()* performs clustering for a number of *k* values and outputs a list of "silhouette" class structures that can be displayed with *fviz_silhouette()* function from the package **factoextra**.
```{r fig.width = 7}
set.seed(1234737)
sil = clustra_sil(data, k = c(2, 3, 4), verbose = TRUE)
## TODO add sampling option if too many ids
splots = lapply(sil, function(x) print(factoextra::fviz_silhouette(x)))
```
Another way to select the number of clusters is the Rand Index comparing different random starts and different number of clusters. When we replicate clustering with different random seeds, the "replicability" is an indicator of how stable the results are for a given k, the number of clusters. For this demonstration, we look at *k = c(2, 3, 4)*, and 10 replicates for each *k*. Note that this uses just random starts rather than "best of 4 random starts" that the *clustra()* function would use because we want to evaluate raw stability. We plot the resulting matrix of Rand Indices.
```{r fig.width = 7, fig.height=7}
set.seed(1234737)
ran = clustra_rand(data, k = c(2, 3, 4), replicates = 10, verbose = TRUE)
rand_plot(ran)
```
The plot shows similarity level between all pairs of 30 clusterings (10 random starts for each of 2, 3, and 4 clusters). The ten random starts agree the most for *k*=3.
```{r}
## restore RNG
RNGkind(rng_prev[1])
```
## KDI installation notes for stringi on linux VM or on Murphy login node
First *wget http://manage01.kdi.local/temp/icudt61l.zip*,
then start R in the same directory and 
*install.packages("stringi", configure.vars="ICUDT_DIR=.")*.

It is useful to have a .Rprofile in your home directory that contains
*options(repos = "http://manage01.kdi.local/CRAN")* for automated access to the internal CRAN. 

Consider export R_LIBS=/usr/me/local/R/library to enable a personal package installation location (replace *me* with your uid).

## KDI installation notes for mvpia:
Package *nloptr* seems to depend on R 4.0 and fails to install under R 3.6. The web suggests updating R or using an older version of *nloptr*. So the issue seems to be that the KDI R version is updated less frequently than CRAN, letting some packages get ahead of it.

To use conda for external dependency management requires additional steps before starting R. See internal Getting Started web pages.

More notes to come: What issues will conda bring? Is it possible to make conda a transparent default for R? Like an alias that runs another script?

