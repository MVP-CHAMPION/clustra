---
title: "clustra: clustering trajectories"
author: "George Ostrouchov, Hanna Gerlovin, and David Gagnon"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{clustra}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, setup}
# First, set up a directory where we want to have any output from this vignette.  
playdir = "~/clustra_play"
if(!dir.exists(playdir)) dir.create(playdir)
knitr::opts_knit$set(
  collapse = TRUE,
  comment = "#>",
  root.dir = playdir
)
start_knit = proc.time()
```
The **clustra** package was built to cluster medical trajectories related by an intervention. For example, a number of individuals are started on a specific drug regimen and their blood pressure data is collected for a varying amount of time before and after the start of the medication. Time is recorded as negative before the start and positive after the start, meaning that the trajectories are aligned at zero, irrespective of any reference to calendar time. Time units can be anything (days, weeks, hours, minutes, etc.) as long as they are the same across all patients. Observations can be unequally spaced.  

We begin by generating a data set. The parameters generate *id*s and for each *id*, a random number of observations based on the *Poisson*($\lambda =$ *m_obs*) distribution plus 3. The 3 additional observations are to guarantee one before intervention at time *start*, one at the intervention time 0, and one after the intervention at time *end*. The *start* time is *Uniform(s_range)* and the *end* time is *Uniform(e_range)*. The remaining times are at times Uniform(*start*, *end*). The time units are arbitrary and depend on your application.

We also set *RNGkind* and *seed* for reproducibility. Code below generates the data, looks at a few observations of the generated data, and writes the data to a csv file in our working directory.
```{r}
library(clustra)
cores_alloc = c(e_mc = 1, m_mc = 1, nthreads = 1, blas = 1)
rng_prev = RNGkind("L'Ecuyer-CMRG")
set.seed(1234567)
data = gen_traj_data(n_id = 3000, m_obs = 25, s_range = c(-365, -14),
                     e_range = c(0.5*365, 2*365), noise = c(0, 5))
head(data)
write.table(data, file = "clustra_gen_data.csv")
```
Select a few random *id*s and print their scatterplots.
```{r fig.width = 7, fig.height = 9}
library(ggplot2)
ggplot(data[id %in% sample(unique(data[, id]), 9)],
       aes(x = time, y = response)) + facet_wrap(~ id) + geom_point()
```
Next, cluster the trajectories. Set *k=3*, spline max degrees of freedom to 30, and maximum iterations to 10. *cores* parameters set the number of cores to use in various components of the code. Note that this does not work on Windows operating systems, where it should be left at 1. In the code that follows, we use verbose output to get information from each iteration.
```{r}
set.seed(1234737)
cl = clustra(data, k = 3, fp = list(maxdf = 30, iter = 10),
                   cores = cores_alloc, verbose = TRUE)
```

Next, plot the resulting fit with a sample of data, colored by the cluster value.
```{r fig.width = 7, fig.height = 7}
sdt = data[, group:=factor(..cl$data_group)][sample(nrow(data), 10000)]

np = 100
k = length(cl$tps)
ntime = seq(data[, min(time)], data[, max(time)], length.out = np)
pdata = expand.grid(time = ntime, group = factor(1:k))
pdata = subset(pdata, group %in% which(lengths(cl$tps) > 0))
pred = vector("list", k)
for(i in 1:k) 
  if(is.null(cl$tps[[i]])) {
    pred[[i]] = NULL
  } else {
    pred[[i]] = mgcv::predict.bam(cl$tps[[i]], newdata = list(time = ntime),
                        type = "response")
  }
pdata$pred = do.call(c, pred)
ggplot(pdata, aes(x = time, y = pred, color = group)) + geom_line() +
  geom_point(data = sdt, aes(y = response), pch = ".") 
```
The Rand index for comparing with true_groups is
```{r}
MixSim::RandIndex(cl$data_group, data[, true_group])
```
A perfect score! Let's double the error variance (4*sd) in data generation ...
```{r fig.width = 7, fig.height = 9}
set.seed(1234567)
data2 = gen_traj_data(n_id = 3000, m_obs = 25, s_range = c(-365, -14),
                     e_range = c(60, 2*365), noise = c(0, 20))
iplot = sample(unique(data2$id), 9)
sampobs = match(data2$id, iplot, nomatch = 0) > 0
ggplot(data2[sampobs], aes(x = time, y = response)) + 
  facet_wrap(~ id) + geom_point()
cl = clustra(data2, k = 3, fp = list(maxdf = 30, iter = 10),
             cores = cores_alloc, verbose = TRUE)
MixSim::RandIndex(cl$data_group, data2[, true_group])
```
Next, we try requesting too many clusters in the noisy data.
```{r}
set.seed(1234737)
cl = clustra(data2, k = 20, fp = list(maxdf = 30, iter = 20),
                   cores = cores_alloc, verbose = TRUE)
```
Next, plot the resulting fit with a sample of data, colored by the cluster value.
```{r fig.width = 7, fig.height = 7}
sdt = data2[, group:=factor(..cl$data_group)][sample(nrow(data2), 10000)]

np = 100
k = length(cl$tps)
ntime = seq(data2[, min(time)], data2[, max(time)], length.out = np)
pdata = expand.grid(time = ntime, group = factor(1:k))
pdata = subset(pdata, group %in% which(lengths(cl$tps) > 0))
pred = vector("list", k)
for(i in 1:k) 
  if(is.null(cl$tps[[i]])) {
    pred[[i]] = NULL
  } else {
    pred[[i]] = mgcv::predict.bam(cl$tps[[i]], newdata = list(time = ntime),
                        type = "response")
  }
pdata$pred = do.call(c, pred)
ggplot(pdata, aes(x = time, y = pred, color = group)) + geom_line() +
  geom_point(data = sdt, aes(y = response), pch = ".") 
```
We stopped the process after 20 iterations. When a cluster has too few data points for maximum spline degrees of freedom, it is allocated to the next nearest cluster and the process continues with fewer clusters. When clusters are well-defined in the data, this process can eliminate some unnecessary clusters. The above process nearly converges at `r cl$k_cl` clusters. With this level of variability a few clusters do jump between the original 3 generating true clusters.

Average silhouette value is a way to select the number of clusters and a silhouette plot provides a way for a deeper evaluation (Rouseeuw 1986). As silhouette requires distances between trajectories, this is not possible due to unequal trajectory sampling without fitting a separate model for each id. As a proxy for distance between points, we use point distances to cluster means in the *clustra_sil()* function. The structure returned from the *clustra()* function contains the matrix *loss*, which has all the information needed to construct these proxy silhouette plots. The function *clustra_sil()* performs clustering for a number of *k* values and outputs a list of "silhouette" class structures that can be displayed with *fviz_silhouette()* function from the package **factoextra**. We use the first data set with *noise = c(0, 5)*.
```{r fig.width = 7}
set.seed(1234737)
sil = clustra_sil(data, k = c(2, 3, 4), cores = cores_alloc, verbose = TRUE)
plot_sil = function(x) {
  msil = round(mean(x$silhouette), 2)
  ggplot(x, aes(id, silhouette, color = cluster, fill = cluster)) + geom_col() +
    ggtitle(paste("Average Width:", msil)) +
    scale_x_discrete(breaks = NULL) + scale_y_continuous("Silhouette Width") +
    geom_hline(yintercept = msil, linetype = "dashed", color = "red")
}
lapply(sil, plot_sil)
```
Another way to select the number of clusters is the Rand Index comparing different random starts and different numbers of clusters. When we replicate clustering with different random seeds, the "replicability" is an indicator of how stable the results are for a given k, the number of clusters. For this demonstration, we look at *k = c(2, 3, 4)*, and 10 replicates for each *k*. Note that this uses just random starts rather than "best of 4 random starts" that the *clustra()* function would use because we want to evaluate raw stability. We plot the resulting matrix of Rand Indices.
```{r fig.width = 7, fig.height=7}
set.seed(1234737)
ran = clustra_rand(data, k = c(2, 3, 4), cores = cores_alloc, replicates = 10,
                   verbose = TRUE)
rand_plot(ran)
```
The plot shows similarity level between all pairs of 30 clusterings (10 random starts for each of 2, 3, and 4 clusters). The ten random starts agree the most for *k* = 2 and *k*=3, making it difficult to decide. But considering deviance, we see that seven of the *k*=3 clusters are near the best deviance attainable even with *k* = 4. Also, interestingly, two of the four cluster attempts collapse to 3 clusters. 
```{r}
## restore RNG
RNGkind(rng_prev[1])
cat("clustra knit run time:\n")
print(proc.time() - start_knit)
```
## KDI installation notes for stringi on linux VM
```{r eval = FALSE}
system("wget http://manage01.kdi.local/temp/icudt61l.zip")
conf = paste0("ICUDT_DIR=", system("pwd", intern = TRUE))
install.packages("stringi", configure.vars=conf)
```
It is useful to have `.Rprofile` in your home directory that contains
```{r eval = FALSE}
options(repos = "http://manage01.kdi.local/CRAN")
```
for automated access to the internal CRAN. 

Also, consider 
```{sh eval = FALSE}
export R_LIBS=/usr/me/local/R/library
```
at the unix prompt to enable a personal package installation location (replace *me* with your uid).

## KDI installation notes for mvpia:
Package *nloptr* seems to depend on R 4.0 and fails to install under R 3.6. The web suggests updating R or using an older version of *nloptr*. So the issue seems to be that the KDI R version is updated less frequently than CRAN, letting some packages get ahead of it.

To use conda for external dependency management requires additional steps before starting R. See internal Getting Started web pages.

More notes to come: What issues will conda bring? Is it possible to make conda a transparent default for R? Like an alias that runs another script?

